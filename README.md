<img width="32766" height="25" alt="image" src="https://github.com/user-attachments/assets/f9ab6bcd-e149-4a8e-b4df-554055c8aa32" />
# GRIT - Graph-based Rating Inference over Time
# GRIT  
## Graph-aware Rating Inference over Time  
### Next Contest Rating Prediction

---

## ğŸ“Œ Overview

The **GRIT challenge** evaluates models on the task of predicting the **next contest rating** of competitive programmers.

Each contest is represented as a graph snapshot where:
- Nodes correspond to a subset of participating contestants.
- Edges connect contestants with similar ratings at the time of contest registration.

The task is to predict the `nextRating` of each contestant in an **autoregressive evaluation setting**, where past predictions are used to construct future inputs.

---

# ğŸ“‚ Data
## Data Description
The dataset is based on **5000 contestants from Kaggle**. Multiple preprocessing steps were applied to tailor it to the graph structure and the GRIT challenge.

Each snapshot corresponds to a single contest:
- Nodes represent a subset of participating contestants.
- Edges connect nodes based on their relative ratings when entering the contest.

---

## ğŸ§© Node Features

Each node contains the following information:
- **node_id** â€“ a unique identifier for each node in the dataset
- **handle** â€“ serves as the contestant ID (strings were converted into integers for privacy)
- **oldRating** â€“ contestantâ€™s rating when registering for contest *i*  
- **rating** â€“ contestantâ€™s rating after contest *i*  
- **num_problems_solved** â€“ number of problems solved in contest *i*  
- **nextRating** â€“ contestantâ€™s rating in the next contest they participate in (*i+1*) â†’ **target to be predicted**  
- **participation_gap** â€“ number of contests since the contestantâ€™s last participation  
- **contestant_count** â€“ total number of contestants in contest *i*

---

## ğŸ”— Edge Construction

Edges are constructed according to the following criteria:
Two nodes *(u, v)* are connected if:
|oldRating_u - oldRating_v| < Î”

- The value of **Î” (delta)** was selected such that the number of edges in any snapshot is less than 30,000.
- If a node does not satisfy the above condition with any other node, it is connected to up to **three nodes** with the smallest rating difference (when possible), to avoid isolated nodes.

---

# ğŸ— Training Data Structure

Each contest snapshot is represented by:
- An adjacency matrix  
- A feature matrix  

Inside the `training/` folder:

### `nodes.parquet`

Used to construct the **feature matrix** for each contest snapshot.

Format:
node_id, handle, contestId,	oldRating, rating,	problems_solved_num, contestants_count, nextRating

### `edges.parquet`

Used to construct the **adjacency matrix** for each contest snapshot.

Format:
contest_id, src, dst

---

# ğŸ§ª Testing & Evaluation

Model evaluation is performed in an **autoregressive manner**.

---

## First Appearance in Test Set

For contestants appearing for the first time in the testing data:

- All features except `nextRating` are available.

---

## Subsequent Appearances

When a contestant appears again in the testing data, the following features will be set to **-1** (invalid rating):

- `oldRating`
- `rating`

These values must be filled using your modelâ€™s previous predictions:

- `rating` at contest *(iâˆ’1)* â†’ becomes `oldRating` at contest *(i)*  
- `nextRating` at contest *(iâˆ’1)* â†’ becomes `rating` at contest *(i)*  

---

### ğŸ’¡ Implementation Hint

You may maintain two dictionaries:

- `old_rate[handle]`
- `current_rate[handle]`

Where:

- `old_rate` stores rating at *(iâˆ’1)*
- `current_rate` stores predicted `nextRating` at *(iâˆ’1)*

At each contest snapshot in the test set:

- Update these values for participating contestants.
- Use them to fill missing features before making predictions.

---

# ğŸ“Š Evaluation Metric

## Mean Absolute Error (MAE)
MAE = (1/N) * Î£ |y_true âˆ’ y_pred|

---

# âš ï¸ Why is GRIT Challenging?

## 1ï¸âƒ£ Missing Values

In a few samples, the raw dataset included rating changes but did not include the number of solved problems.

This results in rare cases where:
rating > oldRating
num_problems_solved = 0

Models must be resilient to these noisy samples.

---

## 2ï¸âƒ£ Inconsistent Participation

Some contestants participate irregularly.

The model must account for the `participation_gap` when predicting the next rating.

---

## 3ï¸âƒ£ Registration Rules

Contestants register for contests before they start.

As long as a contestant registers, they are considered a participant, and the contest affects their rating.

If a contestant registers but does not solve any problems, their rating may drop as if they participated and solved none.

---

## 4ï¸âƒ£ Error Accumulation

Because evaluation is autoregressive:

- Prediction errors propagate forward.
- Mistakes in early contests affect later predictions.
- Models must remain stable over long horizons.

---

# ğŸ¯ Goal

Develop a model that:

- Leverages graph structure within contests  
- Handles irregular participation  
- Remains stable under autoregressive rollout  
- Minimizes MAE on the test set  

---

## 2. Repository Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ train_edges.csv
â”‚   â”‚   â”œâ”€â”€ train_labels.csv
â”‚   â”‚   â”œâ”€â”€ val_edges.csv
â”‚   â”‚   â”œâ”€â”€ val_labels.csv
â”‚   â”‚   â”œâ”€â”€ test_edges.csv
â”‚   â”‚   â”œâ”€â”€ test_nodes.csv
â”‚   â”‚   â””â”€â”€ sample_submission.csv
â”‚   â””â”€â”€ private/
â”‚       â””â”€â”€ test_labels.csv   # never committed (used only in CI)
â”œâ”€â”€ competition/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ validate_submission.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ inbox/
â”œâ”€â”€ leaderboard/
â”‚   â”œâ”€â”€ leaderboard.csv
â”‚   â””â”€â”€ leaderboard.md
â””â”€â”€ .github/workflows/
    â”œâ”€â”€ score_submission.yml
    â””â”€â”€ publish_leaderboard.yml
```

---

## 3. Submission Format

Participants submit a **single CSV file**:

**predictions.csv**
```
id,y_pred
n0001,0.92
n0002,0.13
...
```

Rules:
- `id` must match exactly the IDs in `test_nodes.parquet`
- One row per test node
- `y_pred` must be a float in [0,1]
- No missing or duplicate IDs


---

## 4. How to Submit

1. Fork this repository
2. Create a new folder:
```
submissions/inbox/<team_name>/<run_id>/
```
3. Add:
   - `predictions.csv`
   - `metadata.json`

Example `metadata.json`:
```json
{
  "team": "example_team",
  "model": "llm-only",
  "llm_name": "gpt-x",
}
```

4. Open a Pull Request to `main`

The PR will be **automatically scored** and the result posted as a comment.

---

## 5. Leaderboard

After a PR is merged, the submission is added to:
- `leaderboard/leaderboard.csv`
- `leaderboard/leaderboard.md`

Rankings are sorted by **descending score**.

---

## 6. Rules

- No external or private data
- No manual labeling of test data
- No modification of evaluation scripts
- Only predictions are submitted

Violations may result in disqualification.
